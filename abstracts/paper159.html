---
layout: abstract
absnum: 159
---
{% raw %}

<div data-nosnippet
    style="display:none"
>

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\let \Hat \hat \)

\(\let \Check \check \)

\(\let \Tilde \tilde \)

\(\let \Acute \acute \)

\(\let \Grave \grave \)

\(\let \Dot \dot \)

\(\let \Ddot \ddot \)

\(\let \Breve \breve \)

\(\let \Bar \bar \)

\(\let \Vec \vec \)

\(\newcommand {\bm }[1]{\boldsymbol {#1}}\)

\(\newcommand {\M }[1]{\mathbf {#1}}\)

</div>

<a id="paper-autopage-1"></a>
<div class="center">

<h2>
On Minimizing Arithmetic and Communication Complexity of Jacobi’s Eigenvalue Method: Review and Beyond
</h2>
</div>
<div class="center">

<p>
Yifu Wang, <span class="underline">James Demmel</span>, Hengrui Luo, Ryan Schneider
</p>
</div>
<div class="center">

<p>
Abstract
</p>
</div>

<p>
Jacobi’s method iteratively computes the eigenvalues and eigenvectors of a symmetric matrix. Remarkably simple to implement, Jacobi’s method is a compelling candidate for use on large-scale applications. On the other hand, matrix multiplication
is fundamental in numerical linear algebra, often regarded as a building block for other matrix computations.
</p>

<p>
With these in mind, we establish theoretical bounds on the asymptotic complexity of Jacobi’s method in both arithmetic and communication, aiming for eﬀiciency comparable to matrix multiplication.
</p>

<p>
We not only analyze the complexity of sequential and parallel Jacobi using classical \(O(n^3)\) matrix multiplication, but also introduce recursive Jacobi’s methods that leverage Strassen-like \(O(n^{\omega _0})\) matrix multiplication to achieve
optimal arithmetic and communication lower bounds. We also offer rigorous proofs of convergence for the recursive algorithms. The main contributions are as follows:
</p>
<ul class="enumerate" style="list-style-type:none">

<li>
<p>
<span class="listmarker">1.</span> Starting from a dense real symmetric matrix \(\M {A}\in \M {M}_n(\mathbb {R})\) (without loss of generality, we only consider the real case), the <b>Classical Jacobi’s method</b> sequentially rotates all
off-diagonal entries of \(\M {A}\) in some given ordering. We denote one sweep as rotating through all off-diagonal entries of \(\M {A}\) once. Since Classical Jacobi almost always converges, we assume that the algorithm converges in \(O(1)\)
sweeps and the corresponding total arithmetic cost is \(O(1)\cdot \Theta (n^{3}) = \Theta (n^{3})\).
</p>
<p>
For estimating the lower bound on the communication cost, assume for now that we could only change the ordering of rotations. We denote the size of fast memory by \(M\). Then when \(M^{1/2}&lt;n&lt;M\), we can attain a lower bound of
\(\Omega (n^4/M)\) reads and writes to slow memory, asymptotically exceeding the \(O(n^3/\sqrt {M})\) cost of classical matrix multiplication. To attain the cost of matrix multiplication requires more changes to the algorithm.
</p>
</li>
<li>

<p>
<span class="listmarker">2.</span> Allowing ourselves more freedom than just choosing the ordering of to-be-rotated entries, we next consider the <b>Block Jacobi’s method</b>, in which we rotate \(2b\)-by-\(2b\) blocks instead of one
off-diagonal entry each time. We still assume \(O(1)\) sweeps for the algorithm to converge and choose \(b\) to be able to fit three \(2b\)-by-\(2b\) sub-matrices into the fast memory, i.e. \(b=\Theta (\sqrt {M})\). In this case, the algorithm
attains the communication lower bound \(\Omega (n^{3}/\sqrt {M})\) with \(O(n^{3})\) matrix multiplication.
</p>
</li>
<li>

<p>
<span class="listmarker">3.</span> The highlight of this paper is the <b>Recursive Jacobi’s method</b> we introduce, along with a series of its variations. To the best of our knowledge, this is the first work which can asymptotically attain
the arithmetic and communication costs of Strassen-like matrix multiplication, including a convergence proof.
</p>
<p>
We first propose a “vanilla” recursive algorithm, in which we apply a divide-and-conquer strategy, where the algorithm recursively partitions the input \(n\)-by-\(n\) matrix into smaller \(2b\)-by-\(2b\) blocks, until the size of the to-be-rotated
sub-matrices reach a certain threshold, where \(b=n^f\) and \(0 &lt; f &lt; 1\) is the block parameter. We show that under the assumption that the outermost sweep is executed \(O(1)\) times, the arithmetic complexity is \(F(n)=O(\frac {\log
\log n}{-\log f}\cdot n^{3(1-f)+\omega _{0}f})\), which asymptotically approaches \(O(n^{\omega _{0}})\) as \(f\) approaches \(1\).
</p>
<p>
Convergence analysis for Jacobi’s methods has been widely discussed, taking into account various pivoting strategies (such as rotation orderings and the choice between block and cyclic) as well as processing architectures (sequential or parallel). We
refer readers to [7, 8, 10, 14] for further details. A key ingredient in [7] towards convergence of Classical Jacobi is to restrict the rotation angles of off-diagonal entries in a proper open subset of \((-\frac {\pi }{2},\frac {\pi }{2})\). An analog
for block Jacobi is uniformly bounded cosine transformations [6]. By reordering the columns of the orthogonal rotation matrix \(\M {Q}\) via applying QR decomposition with column pivoting (QRCP for short) to the first-half leading rows of \(\M
{Q}\), [6] successfully proved convergence for the block cyclic Jacobi. We leverage this idea and introduce our first variant of recursive Jacobi with convergence guarantee, the <b>Recursive Jacobi with QRCP</b>. With a slight trade-off between
optimal arithmetic complexity lower bound and convergence guarantee, the recursive Jacobi with QRCP, within \(O(1)\) sweeps, can achieve
</p>
<span class="hidden"> \(\seteqnumber{0}{}{0}\)</span>
<!--
                                                                                                                 {
                                                                                                                   O( log log n
                                                                                                                       − log f
                                                                                                                                · n3(1−f )+ω0 f ), 0 < f ≤ 4−ω
                                                                                                                                                             1
                                                                                                        F (n) =                                                0
                                                                                                                   O( − log f · n
                                                                                                                      log log n    2+f
                                                                                                                                       ),            1
                                                                                                                                                   4−ω0
                                                                                                                                                        < f <  1
-->
<p>

\begin{equation*}
F(n) = \begin{cases} O(\frac {\log \log n}{-\log f}\cdot n^{3(1-f)+\omega _{0}f}), &amp; 0&lt;f\leq \frac {1}{4-\omega _{0}} \\ O(\frac {\log \log n}{-\log f}\cdot n^{2+f}), &amp; \frac {1}{4-\omega _{0}}&lt;f&lt;1
\end {cases}
\end{equation*}

</p>
<p>
due to the \(O(n^3)\) expensive complexity of QRCP.
</p>
<p>
The key of ensuring convergence in [6, 7] is to bound the cosines of rotation angles away from zero, which could also be done by applying LU decomposition with partial pivoting (LUPP for short). Unlike QRCP, LU decomposition can be
implemented recursively with complexity of \(O(n^{\omega _0})\) [2, Section 4.2], and adding partial pivoting to the algorithm doesn’t increase the arithmetic complexity. By applying LUPP to the transpose of the first-half leading columns of \(\M
{Q}\), we introduce the <b>Recursive Jacobi with LUPP</b> which enjoys both optimal arithmetic complexity and convergence.
</p>
<p>
In the sequential case, for \(2&lt;\omega _{0}\leq 3\), the recursive Jacobi is shown to analogously get close to attaining the expected communication lower bound \(\Omega (n^{\omega _{0}}/M^{\omega _{0}/2-1})\) [13]. In practical terms,
recursive Jacobi should be considered as a “galactic algorithm” since the size \(n\) where the algorithm shows benefits grows rapidly as \(f\) approaches 1.
</p>
</li>
<li>

<p>
<span class="listmarker">4.</span> In addition to the sequential cases, we also studied <b>parallel block Jacobi</b> with \(O(n^{3})\) matrix multiplication, in which the algorithm simultaneously rotates off-diagonal blocks in different
columns and rows [1, 9, 12]. We store the \(n\)-by-\(n\) matrix \(\M {A}\) on a \(\sqrt {P}\times \sqrt {P}\) grid of \(P\) processors, with block sizes \(b=n/\sqrt {P}\), which we assume to be an integer for simplicity. Under this scenario, the
arithmetic complexity is \(O(n^{3}/P)\), which demonstrates the optimal linear speedup, and the communication complexity is \(O(n^{2}/\sqrt {P})\) words and \(O(\sqrt {P}\log P)\) messages, which attains the communication lower bound
(except for the \(\log P\) factor) for classical matrix multiplication using the minimum amount of memory.
</p>
</li>
</ul>

<p>
One remark is that the above studies and estimates readily extend to the SVD due to its strong connection with Jacobi’s method [4, 5]. Furthermore, by not restricting ourselves to Jacobi-like methods, our recursive algorithm technique can also
benefit non-Jacobi methods, for example combined with QDWH (QR-based dynamically weighted Halley algorithm) [11].
</p>

<p>
Additionally, since all our recursive algorithms follow a divide-and-conquer paradigm utilizing \(O(n^{\omega _{0}})\) matrix multiplication, it follows from the analysis in [2, 3] that all the proposed algorithms are backward stable.
</p>

<p>
In conclusion:
</p>
<ul class="enumerate" style="list-style-type:none">

<li>
<p>
<span class="listmarker">1.</span> We have demonstrated an asymptotic approach to make the Jacobi’s eigenvalue method and SVD nearly as fast as matrix multiplication, in terms of both arithmetic and communication complexity, across
several scenarios. For \(O(n^3) \) matrix multiplication, we analyzed both sequential and parallel Jacobi’s methods.
</p>
<p>
A remaining open question is whether the (better) lower bound and communication complexity for matrix multiplication using more than the minimum memory is attainable for Jacobi.
</p>
</li>
<li>

<p>
<span class="listmarker">2.</span> For \(O(n^{\omega _0}) \) matrix multiplication, we introduced a series of recursive Jacobi’s methods, focusing on minimizing arithmetic cost while also ensuring the convergence of the proposed algorithms.
</p>
<p>
Another remaining open question is whether these asymptotically faster recursive Jacobi’s methods can be parallelized and attain both the arithmetic and communication complexity lower bounds of matrix multiplication.
</p>
</li>
</ul>
<!--
...... section References ......
-->
<h4 id="autosec-5">References</h4>
<a id="paper-autopage-5"></a>


<ul class="list" style="list-style-type:none">

<li>
<p>
<span class="listmarker">[1]&#x2003;</span> M.&nbsp;Berry and A.&nbsp;Sameh. An overview of parallel algorithms for the singular value and symmetric eigenvalue problems. <i>J. Comp. Appl. Math.</i>, 27:191–213, 1989.
</p>
</li>
<li>

<p>
<span class="listmarker">[2]&#x2003;</span> James Demmel, Ioana Dumitriu, and Olga Holtz. Fast linear algebra is stable. <i>Numerische Mathematik</i>, 108(1):59–91, 2007.
</p>
</li>
<li>

<p>
<span class="listmarker">[3]&#x2003;</span> James Demmel, Ioana Dumitriu, Olga Holtz, and Robert&nbsp;D. Kleinberg. Fast matrix multiplication is stable. <i>Numerische Mathematik</i>, 106:199–224, 2006.
</p>
</li>
<li>

<p>
<span class="listmarker">[4]&#x2003;</span> K.&nbsp;Drmač and K.&nbsp;Veselić. New fast and accurate Jacobi SVD algorithm, I. <i>SIAM J. Mat. Anal. Appl.</i>, 29(4):1322–1342, 2008.
</p>
</li>
<li>

<p>
<span class="listmarker">[5]&#x2003;</span> K.&nbsp;Drmač and K.&nbsp;Veselić. New fast and accurate Jacobi SVD algorithm, II. <i>SIAM J. Mat. Anal. Appl.</i>, 29(4):1343–1362, 2008.
</p>
</li>
<li>

<p>
<span class="listmarker">[6]&#x2003;</span> Zlatko Drmač. A Global Convergence Proof for Cyclic Jacobi Methods with Block Rotations. <i>SIAM Journal on Matrix Analysis and Applications</i>, 31(3):1329–1350, 2010.
</p>
</li>
<li>

<p>
<span class="listmarker">[7]&#x2003;</span> G.&nbsp;E. Forsythe and P.&nbsp;Henrici. The Cyclic Jacobi Method for Computing the Principal Values of a Complex Matrix. <i>Transactions of the American Mathematical Society</i>,
94(1):1–23, 1960.
</p>
</li>
<li>

<p>
<span class="listmarker">[8]&#x2003;</span> V.&nbsp;Hari. Convergence to diagonal form of block Jacobi-type methods. <i>Numer. Math.</i>, 129:449–481, 2015.
</p>
</li>
<li>

<p>
<span class="listmarker">[9]&#x2003;</span> Franklin&nbsp;T. Luk and Haesun Park. A Proof of Convergence for Two Parallel Jacobi SVD Algorithms. <i>IEEE Trans. Computers</i>, 38:806–811, 1989.
</p>
</li>
<li>

<p>
<span class="listmarker">[10]&#x2003;</span> W.&nbsp;Mascarenhas. Convergence of the Jacobi method for arbitrary orderings. <i>SIAM J. Mat. Anal. Appl.</i>, 16(4):1197–1209, Oct 1995.
</p>
</li>
<li>

<p>
<span class="listmarker">[11]&#x2003;</span> Yuji Nakatsukasa and Nicholas&nbsp;J. Higham. Stable and Eﬀicient Spectral Divide and Conquer Algorithms for the Symmetric Eigenvalue Decomposition and the SVD. <i>SIAM Journal on
Scientific Computing</i>, 35(3):A1325–A1349, 2013.
</p>
</li>
<li>

<p>
<span class="listmarker">[12]&#x2003;</span> A.&nbsp;Sameh. On Jacobi and Jacobi-like algorithms for parallel computers. <i>Math. Comp.</i>, 25(115):579–590, July 1971.
</p>
</li>
<li>

<p>
<span class="listmarker">[13]&#x2003;</span> Jacob Scott. <i>An I/O-Complexity Lower Bound for All Recursive Matrix Multiplication Algorithms by Path-Routing</i>. PhD thesis, UC Berkeley Mathematics PhD thesis, 2015.
</p>
</li>
<li>

<p>
<span class="listmarker">[14]&#x2003;</span> G.&nbsp;Shroff and R.&nbsp;Schreiber. On the convergence of the cyclic Jacobi method for parallel block orderings. <i>SIAM J. Mat. Anal. Appl.</i>, 10(3):326–346, 1989 1989.
</p>
<p>

</p>
</li>
</ul>

{% endraw %}
