---
layout: abstract
absnum: 39
---
{% raw %}

<div data-nosnippet
    style="display:none"
>

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

</div>

<a id="paper-autopage-1"></a>
<div class="center">

<h2>
DATA-PARALLEL adaptive TENSOR-TRAIN CROSS approximation
</h2>
</div>
<div class="center">

<p>
<span class="underline">Tianyi Shi</span>, Daniel Hayes, Jing-Mei Qiu
</p>
</div>
<div class="center">

<p>
Abstract
</p>
</div>

<p>
The tensor-train (TT) format is a low rank tensor representation frequently used for high order tensors. Traditionally, the TT format is computed directly with all the elements in the tensor. In this talk, we propose a TT decomposition algorithm
that partitions the tensor into subtensors and performs decomposition individually before merging back together. This factorization routine is ideal for distributed memory parallelism. In addition, instead of computing the TT format with singular
value decomposition based techniques, our proposed method, parallel adaptive TT cross, is a data-centric iterative method based on data skeletonization and has a low computational cost. In particular, our method is based on two innovative iterative
formulations for data extraction and TT format construction, and we provide theoretical guarantees, communication analysis, and scaling results. For example, strong scaling results on synthetic datasets and discretized solutions of 2D and 3D
Maxwellian equations suggest that this algorithm scales well with the number of computing cores, with respect to both storage and timing. This talk is based on the preprint <a href="https://arxiv.org/abs/2407.11290" target="_blank"
>https://arxiv.org/abs/2407.11290</a>
</p>

<p>
BIO: Tianyi Shi is a postdoctoral fellow in the Scalable Solvers Group in Applied Mathematics and Computational Research Division at Lawrence Berkeley National Laboratory. He obtained his PhD from the Center for Applied Mathematics at
Cornell University. His research interests include numerical linear algebra with a focus on sparse and data-sparse matrices and tensors, and high-performance computing.
</p>

{% endraw %}
