---
layout: abstract
absnum: 172
---
{% raw %}

<div data-nosnippet
    style="display:none"
>

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

</div>

<a id="paper-autopage-1"></a>
<div class="center">

<h2>
Krylov Methods and Polynomials
</h2>
</div>
<div class="center">

<p>
<span class="underline">Ron Morgan</span>
</p>
</div>
<div class="center">

<p>
Abstract
</p>
</div>

<p>
It is well-known that all Krylov methods have polynomials underlying them. Here these polynomials are used in new ways to develop iterative methods. There are several applications including systems of linear equations with multiple right-hand sides.
</p>
<!--
...... section Polynomial Preconditioning ......
-->
<h4 id="autosec-5"><span class="sectionnumber">1&#x2003;</span>Polynomial Preconditioning</h4>
<a id="paper-autopage-5"></a>


<p>
Polynomial preconditioning goes way back, see for instance&nbsp;[6, 11, 10, 2, 12], but has never become standard. In&nbsp;[5, 8], polynomial preconditioning is made a more practical method with these improvements:
</p>
<ul class="enumerate" style="list-style-type:none">

<li>
<p>
<span class="listmarker">1.</span> The polynomial is easy to determine. It is generated by GMRES instead of from eigenvalue estimates as some approaches have done.
</p>

</li>
<li>

<p>
<span class="listmarker">2.</span> The implementation is eﬀicient due to using roots of the GMRES residual polynomial.
</p>

</li>
<li>

<p>
<span class="listmarker">3.</span> The method is more stable than some other approaches. This is from the implementation with roots instead of coeﬀicients. Also, additional stability control comes from adding extra copies of outstanding roots.
</p>
</li>
</ul>

<p>
With polynomial preconditioning, \(Ax = b\) becomes
</p>

<span class="hidden"> \(\seteqnumber{0}{}{0}\)</span>

<!--


                                                                                                 Ap(A)y = b,     x = p(A)y.--><a id="Eq1"></a><!--                                                                                                    (1)

-->

<p>

\begin{equation}
Ap(A)y = b, \ \ \ \ x = p(A)y. \label {Eq1}
\end{equation}

</p>

<p>
Polynomial preconditioning works because it transforms the spectrum of \(A\) into a better spectrum. Another reason why it is effective is that there is more power per iteration and much less orthogonalization. Also, there is greater opportunity for
parallelism. However, polynomial preconditioning may not needed for fairly easy systems.
</p>

<p>
Example Polynomial preconditioning is used for a matrix from a biharmonic differential equation. A degree 200 polynomial is applied, then a degree 50 polynomial is used along with an incomplete factorization preconditioner. Table 1 first shows that
polynomial preconditioning can be very effective for the diﬀicult original system of equations. The time is reduced from hours to under a minute. The problem is easier when standard incomplete factorization preconditioning is applied. Then the
effect of the polynomial preconditioning is not as dramatic, but it is still helpful. The time goes from over 3 minutes to below 9 seconds. See&nbsp;[8] for more results.
</p>

<figure id="autoid-1" class="table ">

<div class="figurecaption">
<p>
Table&nbsp;1:&nbsp;Polynomial preconditioning applied to a biharmonic matrix of size \(n=40{,}000\) from the differential equation \(- u_{xxxx} - u_{yyyy} + u_{xxx} = f\) on the unit square. IF stands for incomplete factorization with no
fill-in after shifting the matrix by \(0.5*I\).
</p>
</div>
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>

<tr class="hline">
<td class="tdc tvertbarl tvertbarr" style="border-top: 4px double; border-left: 1px solid black; border-right: 1px solid black">Method</td>
<td class="tdc tvertbarr" style="border-top: 4px double; border-right: 1px solid black">GMRES(50)</td>
<td class="tdc tvertbarr" style="border-top: 4px double; border-right: 1px solid black">PP(200)-GMRES(50)</td>
<td class="tdc tvertbarr" style="border-top: 4px double; border-right: 1px solid black">IF-GMRES(50)</td>
<td class="tdc tvertbarr" style="border-top: 4px double; border-right: 1px solid black">PP(50)-IF-GMRES(50)</td>
</tr>

<tr class="hline">
<td class="tdc tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">Time</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">14.6 hours</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">55 seconds</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">3.5 minutes</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">8.5 seconds</td>
</tr>

<tr class="hline" aria-hidden="true">
<td class="tdc" style="border-top: 4px double"></td>
<td class="tdc" style="border-top: 4px double"></td>
<td class="tdc" style="border-top: 4px double"></td>
<td class="tdc" style="border-top: 4px double"></td>
<td class="tdc" style="border-top: 4px double"></td>
</tr>
</table>

</div>

</figure>
<!--
...... section Polynomial Approximation of ......
-->
<h4 id="autosec-8"><span class="sectionnumber">2&#x2003;</span>Polynomial Approximation of \(A^{-1}\)</h4>
<a id="paper-autopage-8"></a>


<p>
It is surprising that even for a large matrix, it is often possible to find a polynomial \(p\) so that \(p(A)\) is a good approximation to \(A^{-1}\)&nbsp;[4]. We use the \(p\) polynomial from Equation (<a href="paper.html#Eq1">1</a>) that is
generated by GMRES. This is implemented using the roots of the GMRES residual polynomial.
</p>

<p>
It can be proved for the symmetric case that the accuracy of the approximating polynomial follows the residual norm curve. Stability control with added roots is even more important here than for polynomial preconditioning, because a high degree
polynomial is needed.
</p>

<p>
Example: For a convection-diffusion matrix, Table 2 shows that the relative accuracy of the polynomial approximation to the inverse follows the GMRES residual norm. See&nbsp;[4] for more.
</p>

<figure id="autoid-2" class="table ">

<div class="figurecaption">
<p>
Table&nbsp;2:&nbsp;A polynomial approximation is found to the inverse of a matrix of size \(n=2500\) from the convection-diffusion equation \(- u_{xx} - u_{yy} + 2 u_{x} = f\) on the unit square. The starting vector is a random vector normed
to one. Relative accuracy of the polynomial is compared to the GMRES residual norm.
</p>
</div>
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>

<tr class="hline">
<td class="tdc tvertbarl tvertbarr" style="border-top: 4px double; border-left: 1px solid black; border-right: 1px solid black">Degree</td>
<td class="tdc tvertbarr" style="border-top: 4px double; border-right: 1px solid black">GMRES residual norm</td>
<td class="tdc tvertbarr" style="border-top: 4px double; border-right: 1px solid black">\(\frac {\|A^{-1} - p(A)\|} {\|A^{-1}\|}\)</td>
</tr>

<tr class="hline">
<td class="tdc tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">50</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(8*10^{-3}\)</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(2*10^{-1}\)</td>
</tr>

<tr class="hline">
<td class="tdc tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">100</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(4*10^{-5}\)</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(6*10^{-4}\)</td>
</tr>

<tr class="hline">
<td class="tdc tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">150</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(1*10^{-8}\)</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(3*10^{-7}\)</td>
</tr>

<tr class="hline">
<td class="tdc tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">200</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(8*10^{-12}\)</td>
<td class="tdc tvertbarr" style="border-right: 1px solid black">\(3*10^{-10}\)</td>
</tr>

<tr class="hline" aria-hidden="true">
<td class="tdc" style="border-top: 4px double"></td>
<td class="tdc" style="border-top: 4px double"></td>
<td class="tdc" style="border-top: 4px double"></td>
</tr>
</table>

</div>

</figure>

<p>
Applications of polynomial approximation to \(A^{-1}\) include:
</p>
<ul class="enumerate" style="list-style-type:none">

<li>
<p>
<span class="listmarker">1.</span> Systems with multiple right-hand sides&nbsp;[4]. The polynomial approximation that is generated with one right-hand side can be applied to other right-hand sides to solve the systems.
</p>

</li>
<li>

<p>
<span class="listmarker">2.</span> Multilevel Monte Carlo for the trace of the inverse in quantum chromodynamics&nbsp;[7]. Polynomials form the basis for this approach, but deflation of eigenvalues is also crucial.
</p>

</li>
<li>

<p>
<span class="listmarker">3.</span> Functions of matrices (in progress). A polynomial approximation to the function of a matrix can be found by interpolating the function at the harmonic Ritz values (the roots of the GMRES polynomial).
</p>
</li>
</ul>
<!--
...... section Twin CG for multiple right-hand side systems. Also Twin CR, BiCGStab, BiCG, GMRES and QMR. ......
-->
<h4 id="autosec-11"><span class="sectionnumber">3&#x2003;</span>Twin CG for multiple right-hand side systems. Also Twin CR, BiCGStab, BiCG, GMRES and QMR.</h4>
<a id="paper-autopage-11"></a>


<p>
The first application in the previous section used the same polynomial for multiple right-hand sides. Here we again have the same polynomial, but applied in a simpler way. We use the same iteration coeﬀicients for all right-hand sides. When this
approach is used for the conjugate gradient method (CG), we call this “Twin CG for multiple right-hand sides.”
</p>

<p>
As an example, consider two systems, \(Ax^{(1)} = b^{(1)}\) and \(Ax^{(2)} = b^{(2)}\). One step in the CG iteration for the first system is \(x^{(1)} = x^{(1)} + \alpha ^{(1)}*p^{(1)}\). Our approach is to also do \(x^{(2)} = x^{(2)}
+ \alpha ^{(1)}*p^{(2)}\) with the same \(\alpha ^{(1)}\) as for the first system. Similarly, the other CG steps have the same constants for both right-hand sides.
</p>

<p>
The residual polynomial helps explain why this is effective. For the second right-hand side, the residual polynomial is the same as for the first. So if \(r^{(1)} = \pi ^{(1)}(A)b^{(1)}\), then also \(r^{(2)} = \pi ^{(1)}(A)b^{(2)}\). The
polynomial \(\pi ^{(1)}\) needs to be small at the eigenvalues of \(A\) in order for the first system to converge, and this makes the second system converge also. The first right-hand side, \(b^{(1)}\), does need to not be deficient in some
eigencomponents. For the deficient case, creating a first system with a random right-hand side is recommended.
</p>

<p>
With this twin approach for CG, the multiple right-hand systems generally converge together. However, there can be momentary instability due to steep slope of the polynomial at an outstanding eigenvalue. This tends to be quickly automatically
corrected.
</p>

<p>
Remarkable things about Twin CG:
</p>
<ul class="enumerate" style="list-style-type:none">

<li>
<p>
<span class="listmarker">1.</span> The code can be very simple. All \(x\) vectors for all right-hand sides can be grouped together into one matrix, and similarly for \(r\) and \(p\) vectors. Then most operations can be done with matrices
instead of vectors.
</p>

</li>
<li>

<p>
<span class="listmarker">2.</span> This method is extremely parallelizable. All dot products are eliminated except for the first right-hand side (one may need to monitor residual norms for other right-hand sides occasionally after the first system
converges). The matrix-vector products can be done together for all right-hand sides. The DAXPY’s can also be performed together, so they become matrix operations.
</p>

</li>
<li>

<p>
<span class="listmarker">3.</span> Stability control happens automatically due to roundoff error. So roundoff error is essential to the success of the method. This is because the Lanczos algorithm that is the basis for CG develops extra copies of
outstanding eigenvalues&nbsp;[9]. This mimics the addition of roots given in&nbsp;[5, 8] for stability control with polynomial preconditioning. With this Twin CG approach, it is surprising that the extra copy appears almost as soon as stability
control is needed. However, for cases with several outstanding eigenvalues, extra copies will not all appear simultaneously, so we have a way of augmenting with some manual stability control.
</p>

</li>
<li>

<p>
<span class="listmarker">4.</span> Seed methods&nbsp;[3, 1] for deflating eigenvalues can be added. Seeding is done during solution of the first system, then the Twin CG method is applied to the second and subsequent systems. In addition, we
are working on a new seed approach for the case where roundoff error interferes with the accuracy of the seeding&nbsp;[1].
</p>
</li>
</ul>

<p>
Finally, a few comments about nonsymmetric systems:
</p>

<ul class="enumerate" style="list-style-type:none">

<li>
<p>
<span class="listmarker">1.</span> Similar to CG for symmetric systems, a Twin BiCG method can be given for the nonsymmetric case. BiCG has automatic stability control similar to CG.
</p>

</li>
<li>

<p>
<span class="listmarker">2.</span> Twin BiCG uses only half of the matrix-vector products for the second and subsequent right-hand sides as for the first.
</p>

</li>
<li>

<p>
<span class="listmarker">3.</span> BiCGStab does not have the automatic stability control.
</p>

</li>
<li>

<p>
<span class="listmarker">4.</span> The twin approach for multiple right-hand sides can even be used with restarted GMRES. Dot products are eliminated except for the first right-hand side, but the rest of the orthogonalization cost is needed for
each right-hand side system.
</p>
</li>
</ul>
<!--
...... section References ......
-->
<h4 id="autosec-12">References</h4>
<a id="paper-autopage-12"></a>


<ul class="list" style="list-style-type:none">

<li>
<p>
<span class="listmarker">[1]&#x2003;</span> A.&nbsp;M. Abdel-Rehim, R.&nbsp;B. Morgan, and W.&nbsp;Wilcox, <i>Improved seed methods for symmetric positive definite linear equations with multiple right-hand sides</i>, Numer.
Linear Algebra Appl., 21 (2014), pp.&nbsp;453–471.
</p>
</li>
<li>

<p>
<span class="listmarker">[2]&#x2003;</span> S.&nbsp;F. Ashby, <i>Polynomial preconditioning for conjugate gradient methods</i>. PhD Thesis, University of Illinois at Urbana-Champaign, 1987.
</p>
</li>
<li>

<p>
<span class="listmarker">[3]&#x2003;</span> T.&nbsp;F. Chan and W.&nbsp;Wan, <i>Analysis of projection methods for solving linear systems with multiple right-hand sides</i>, SIAM J. Sci. Comput., 18 (1997), pp.&nbsp;1698–1721.
</p>
</li>
<li>

<p>
<span class="listmarker">[4]&#x2003;</span> M.&nbsp;Embree, J.&nbsp;A. Henningsen, J.&nbsp;Jackson, and R.&nbsp;B. Morgan, <i>Polynomial approximation to the inverse of a large matrix</i>, tech. rep., Baylor University, 2024.
Available at https://sites.baylor.edu/ronald_morgan/reports/.
</p>
</li>
<li>

<p>
<span class="listmarker">[5]&#x2003;</span> M.&nbsp;Embree, J.&nbsp;A. Loe, and R.&nbsp;B. Morgan, <i>Polynomial preconditioned Arnoldi with stability control</i>, SIAM J. Sci. Comput., 43 (2021), pp.&nbsp;A1–A25.
</p>
</li>
<li>

<p>
<span class="listmarker">[6]&#x2003;</span> C.&nbsp;Lanczos, <i>Solution of systems of linear equations by minimized iterations</i>, J. Res. Nat. Bur. Standards, 49 (1952), pp.&nbsp;33–53.
</p>
</li>
<li>

<p>
<span class="listmarker">[7]&#x2003;</span> P.&nbsp;Lashomb, R.&nbsp;B. Morgan, T.&nbsp;Whyte, and W.&nbsp;Wilcox, <i>Multi-polynomial Monte Carlo for QCD trace estimation</i>, Comput. Phys. Commun., 300 (2024),
pp.&nbsp;109163–1 – 109163–10.
</p>
</li>
<li>

<p>
<span class="listmarker">[8]&#x2003;</span> J.&nbsp;A. Loe and R.&nbsp;B. Morgan, <i>Toward eﬀicient polynomial preconditioning for GMRES</i>, Numer. Linear Algebra Appl., 29 (2021), pp.&nbsp;1–21.
</p>
</li>
<li>

<p>
<span class="listmarker">[9]&#x2003;</span> C.&nbsp;C. Paige, <i>The computation of eigenvectors and eigenvalues of very large sparse matrices</i>. PhD Thesis, University of London, 1971.
</p>
</li>
<li>

<p>
<span class="listmarker">[10]&#x2003;</span> Y.&nbsp;Saad, <i>Practical use of some Krylov subspace methods for solving indefinite and unsymmetric linear systems</i>, SIAM J. Sci. Stat. Comput., 5 (1984), pp.&nbsp;203–228.
</p>
</li>
<li>

<p>
<span class="listmarker">[11]&#x2003;</span> E.&nbsp;L. Stiefel, <i>Kernel polynomials in linear algebra and their numerical applications</i>, Nat. Bur. Standards, Appl. Math. Ser., 49 (1958), pp.&nbsp;1–22.
</p>
</li>
<li>

<p>
<span class="listmarker">[12]&#x2003;</span> H.&nbsp;K. Thornquist, <i>Fixed-Polynomial Approximate Spectral Transformations for Preconditioning the Eigenvalue Problem</i>, PhD thesis, Rice University, 2006. Technical report TR06-05,
Department of Computational and Applied Mathematics, Rice University.
</p>
<p>

</p>
</li>
</ul>

{% endraw %}
