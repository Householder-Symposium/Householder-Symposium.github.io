---
layout: abstract
absnum: 1
---
{% raw %}

<div data-nosnippet
    style="display:none"
>

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\let \Hat \hat \)

\(\let \Check \check \)

\(\let \Tilde \tilde \)

\(\let \Acute \acute \)

\(\let \Grave \grave \)

\(\let \Dot \dot \)

\(\let \Ddot \ddot \)

\(\let \Breve \breve \)

\(\let \Bar \bar \)

\(\let \Vec \vec \)

\(\require {mathtools}\)

\(\newenvironment {crampedsubarray}[1]{}{}\)

\(\newcommand {\smashoperator }[2][]{#2\limits }\)

\(\newcommand {\SwapAboveDisplaySkip }{}\)

\(\newcommand {\LaTeXunderbrace }[1]{\underbrace {#1}}\)

\(\newcommand {\LaTeXoverbrace }[1]{\overbrace {#1}}\)

\(\newcommand {\LWRmultlined }[1][]{\begin {multline*}}\)

\(\newenvironment {multlined}[1][]{\LWRmultlined }{\end {multline*}}\)

\(\let \LWRorigshoveleft \shoveleft \)

\(\renewcommand {\shoveleft }[1][]{\LWRorigshoveleft }\)

\(\let \LWRorigshoveright \shoveright \)

\(\renewcommand {\shoveright }[1][]{\LWRorigshoveright }\)

\(\newcommand {\shortintertext }[1]{\text {#1}\notag \\}\)

\(\newcommand {\vcentcolon }{\mathrel {\unicode {x2236}}}\)

\(\newcommand {\bm }[1]{\boldsymbol {#1}}\)

\(\newcommand {\C }{\mathbb {C}}\)

\(\DeclareMathOperator {\rank }{rank}\)

\(\DeclareMathOperator {\Range }{Range}\)

</div>

<a id="paper-autopage-1"></a>
<div class="center">

<h2>
Operator Learning without the Adjoint
</h2>
</div>
<div class="center">

<p>
<span class="underline">Nicolas Boullé</span>, Diana Halikias, Samuel Otto, Alex Townsend
</p>
</div>
<div class="center">

<p>
Abstract
</p>
</div>

<p>
There is a mystery at the heart of operator learning: how can one recover a non-self-adjoint operator from data without probing its adjoint? Current practical approaches suggest that one can accurately recover an operator while only using data
generated by the forward action of the operator without access to the adjoint&nbsp;[5]. However, naively, it seems essential to sample the action of the adjoint for learning solution operator of time-dependent partial differential equations
(PDEs)&nbsp;[3]. This motivates a fundamental question in numerical linear algebra: can one approximate a non-symmetric low-rank matrix without sketching its adjoint?
</p>

<p>
In this talk, we will explore the limits of adjoint-free low-rank matrix recovery and propose an approach that could help analyze the behavior of structured matrix recovery algorithms. Then, we will show that one can approximate a family of
non-self-adjoint infinite-dimensional compact operators via projection onto a Fourier basis without querying the adjoint. We will apply the result to recover Green’s functions of elliptic partial differential operators and derive an adjoint-free sample
complexity bound. While existing infinite-dimensional numerical linear algebra theory justifies low sample complexity in operator learning&nbsp;[2, 4], ours is the first adjoint-free analysis that attempts to close the gap between theory and
practice&nbsp;[1].
</p>
<!--
...... paragraph Limits of adjoint-free low-rank matrix recovery. ......
-->


<p>
<span class="paragraph" id="autosec-5">Limits of adjoint-free low-rank matrix recovery.</span>
<a id="paper-autopage-5"></a>
We start in the fundamental setting of recovering a low-rank matrix by querying the map \(x\mapsto Fx\) but without access to \(x\mapsto F^*x\). We show that querying \(x\mapsto F^*x\) is essential for recovering \(F\) and prove rigorous
guarantees on the quality of the reconstruction in terms of how close \(F\) is to a symmetric matrix. Thus, we conclude that without prior knowledge of the properties of the adjoint, one must have access to its action.
</p>

<p>
We assume that \(F\) is \(\delta \)-near-symmetric (<em>i.e.</em>,&nbsp;its left and right singular subspaces are \(\delta \)-close), but we only have access to partial information regarding the symmetry of \(F\), namely that \(F\) is \(\epsilon
\)-near-symmetric for some \(\epsilon \geq \delta \), and sketching constraint \(FX\). To quantify the resulting uncertainty about \(F\), we define the set of possible matrices one could recover given this prior knowledge as
</p>

<span class="hidden"> \(\seteqnumber{0}{}{0}\)</span>

<!--

                                                                                                                                             ∗
                                                                                   ΩϵF,X = {A ∈ Mn (C) : rank(A) = k, AX = F X, ∃Q ∈ O(k), ∥UA VA − Q∥2 ≤ ϵ},                                               (1)--><a id="def_set_omega"></a><!--

-->

<p>

\begin{equation}
\label {def_set_omega} \Omega _{F, X}^\epsilon = \{ A \in M_n(\C )\colon \rank (A) = k,\, AX = FX,\, \exists Q\in O(k),\, \| U_A^\ast V_A - Q \|_2 \leq \epsilon \},
\end{equation}

</p>

<p>
where \(A=U_A S_A V_A^\ast \) is the singular value decomposition of \(A\), \(O(k)\) is the group of \(k\times k\) orthogonal matrices, and \(\|\cdot \|_2\) denotes the spectral norm. Hence, given some tolerance \(\epsilon \), \(\Omega _{F,
X}^\epsilon \) is the set of \(\epsilon \)-near-symmetric matrices that can be returned by any low-rank recovery algorithm when approximating \(F\), such as the randomized SVD&nbsp;[6, 7] or the Nyström method&nbsp;[8].
</p>

<p>
The size of \(\Omega _{F, X}^\epsilon \) is measured by its diameter in the spectral norm and determines the maximum accuracy of any reasonable reconstruction. If the diameter is large, one cannot estimate \(F\) accurately, as one cannot
distinguish between any candidate matrix in \(\Omega _{F, X}^\epsilon \). This is because any matrix in \(\Omega _{F, X}^\epsilon \) satisfies the sketching constraint and is near-symmetric. On the other hand, a small diameter guarantees
the fidelity of the reconstruction. We provide sharp upper and lower bounds on the size of \(\Omega _{F, X}^\epsilon \), i.e., determine how far apart any two matrices in \(\Omega _{F, X}^\epsilon \) can be from each other, with respect to
\(\epsilon \), which measures our prior knowledge of \(F\)’s symmetry. The upper and lower bounds on the diameter of \(\Omega _{F, X}^\epsilon \) reveal that the uncertainty about \(F\) given queries of its action is directly related to the
uncertainty about the symmetry of its left and right singular subspaces. For example, our ability to recover a symmetric rank-\(k\) matrix using \(k \leq s &lt; n\) queries is fundamentally limited by our prior knowledge about the proximity of
\(\Range (F)\) and \(\Range (F^*)\) because there are many asymmetric matrices with the same rank that satisfy the same sketching constraints. This result is a fundamental limitation of adjoint-free low-rank matrix recovery in numerical linear
algebra and has implications for operator learning.
</p>
<!--
...... paragraph An adjoint-free operator learning approach. ......
-->


<p>
<span class="paragraph" id="autosec-6">An adjoint-free operator learning approach.</span>
<a id="paper-autopage-6"></a>
To provide an operator learning approach that does not need access to the adjoint, we exploit regularity results from PDE theory to estimate the range of the adjoint of the solution operator. This allows us to prove the first guarantees on the
accuracy of adjoint-free approximations. Our key insight is to leverage the favorable properties of a prior self-adjoint operator, such as the Laplace–Beltrami operator, to use as an operator preconditioner in the approximation problem. In particular,
we query the action of the solution operator on the eigenfunctions of the prior self-adjoint operator, yielding an approximation with an error that decays at a rate determined by the eigenvalues of the prior. This is remarkable because common
operator learning techniques always seem to plateau; yet, we construct a simple algorithm that provably converges.
</p>
<!--
...... paragraph The effect of non-normality on sample complexity. ......
-->


<p>
<span class="paragraph" id="autosec-7">The effect of non-normality on sample complexity.</span>
<a id="paper-autopage-7"></a>
We derive a sample complexity bound for our algorithm when applied to second-order uniformly-elliptic PDEs that are perturbed away from self-adjointness by lower-order terms. We show that for small perturbations, our bound on the
approximation error grows linearly with the size of the perturbation, and we conjecture that this linear growth continues for large perturbations as well. This aspect of the error growth is also present in common operator learning techniques, as our
numerical experiments illustrate. With respect to our operator learning algorithm, this means that the number of samples required to achieve a fixed error tolerance grows algebraically with the perturbation size.
</p>
<!--
...... section References ......
-->
<h4 id="autosec-8">References</h4>
<a id="paper-autopage-8"></a>


<ul class="list" style="list-style-type:none">

<li>
<p>
<span class="listmarker">[1]&#x2003;</span> N.&nbsp;Boullé, D.&nbsp;Halikias, S.&nbsp;E. Otto, and A.&nbsp;Townsend, <i>Operator learning without the adjoint</i>, arXiv preprint arXiv:2401.17739, (2024).
</p>
</li>
<li>

<p>
<span class="listmarker">[2]&#x2003;</span> N.&nbsp;Boullé, D.&nbsp;Halikias, and A.&nbsp;Townsend, <i>Elliptic PDE learning is provably data-eﬀicient</i>, Proc. Natl. Acad. Sci. USA, 120 (2023), p.&nbsp;e2303904120.
</p>
</li>
<li>

<p>
<span class="listmarker">[3]&#x2003;</span> N.&nbsp;Boullé, S.&nbsp;Kim, T.&nbsp;Shi, and A.&nbsp;Townsend, <i>Learning Green’s functions associated with time-dependent partial differential equations</i>, J. Mach. Learn. Res., 23
(2022), pp.&nbsp;1–34.
</p>
</li>
<li>

<p>
<span class="listmarker">[4]&#x2003;</span> N.&nbsp;Boullé and A.&nbsp;Townsend, <i>Learning elliptic partial differential equations with randomized linear algebra</i>, Found. Comput. Math., 23 (2023), pp.&nbsp;709–739.
</p>
</li>
<li>

<p>
<span class="listmarker">[5]&#x2003;</span> N.&nbsp;Boullé and A.&nbsp;Townsend, <i>A mathematical guide to operator learning</i>, in Handbook of Numerical Analysis, vol.&nbsp;25, Elsevier, 2024, ch.&nbsp;3, pp.&nbsp;83–125.
</p>
</li>
<li>

<p>
<span class="listmarker">[6]&#x2003;</span> N.&nbsp;Halko, P.-G. Martinsson, and J.&nbsp;A. Tropp, <i>Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</i>, SIAM Rev.,
53 (2011), pp.&nbsp;217–288.
</p>
</li>
<li>

<p>
<span class="listmarker">[7]&#x2003;</span> P.-G. Martinsson and J.&nbsp;A. Tropp, <i>Randomized numerical linear algebra: Foundations and algorithms</i>, Acta Numer., 29 (2020), pp.&nbsp;403–572.
</p>
</li>
<li>

<p>
<span class="listmarker">[8]&#x2003;</span> E.&nbsp;J. Nyström, <i>Über die praktische Auflösung von Integralgleichungen mit Anwendungen auf Randwertaufgaben</i>, Acta Math., 54 (1930), pp.&nbsp;185–204.
</p>
<p>

</p>
</li>
</ul>

{% endraw %}
